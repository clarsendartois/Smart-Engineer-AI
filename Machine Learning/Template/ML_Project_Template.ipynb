{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4fe54779",
      "metadata": {
        "id": "4fe54779"
      },
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://colab.research.google.com/github/clarsendartois/Smart-Engineer-AI/blob/main/Machine%20Learning/Template/ML_Project_Template.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "# üì∂ **A Step-by-Step Field Guide for Building Machine Learning Projects: Reusable Blueprint**\n",
        "\n",
        "> _‚ÑπÔ∏è This template provides a structured approach to undertaking any Machine Learning project, from initial problem definition to deployment and continuous monitoring. Use this as your personal blueprint to ensure no critical step is missed. Fill in the blank spaces with your project-specific details and code!_ ‚ú®\n",
        "\n",
        "<center>\n",
        "  <a href=\"https://www.youtube.com/watch?v=astmDMRHgds\" target=\"_blank\">\n",
        "  <img alt='Thumbnail for a video showing 3 AI-powered Google Colab features' src=\"https://i9.ytimg.com/vi_webp/DbjnrIa56DA/mqdefault.webp?v=6832654a&sqp=COTS68EG&rs=AOn4CLBjtg2e9ps9OALR55GWV3BbKzkKLg\" height=\"188\" width=\"336\">\n",
        "  </a>\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "# **General Best Practices and Considerations ‚ú®**\n",
        "* **Version Control:** ‚ÜîÔ∏è Use Git for code, data, and model versions.\n",
        "  * **Repo Link:** [Link to your GitHub/GitLab repo]\n",
        "* **Documentation:** üìù Document every step, from problem definition to deployment details.\n",
        "  * **Documentation Location:** [e.g., README.md, Confluence page]\n",
        "* **Reproducibility:** ‚ôªÔ∏è Ensure your experiments and results can be reproduced by others.\n",
        "  * **How:** [e.g., \"Using requirements.txt for dependencies, fixed random seeds.\"]\n",
        "* **Collaboration:** ü§ù Work effectively in a team, leveraging tools and clear communication.\n",
        "  * **Team Communication Tools:** [e.g., Slack, Microsoft Teams]\n",
        "* **Ethical Considerations:** ü§î Address potential biases in data and models, fairness, and privacy.\n",
        "  * **Mitigation Strategies:** [e.g., \"Fairness metrics checked for disparate impact on demographic groups.\"]\n",
        "* **Cost Management:** üí≤ Be mindful of computational costs, especially in cloud environments.\n",
        "  * **Cost Optimization:** [e.g., \"Using spot instances for training, optimizing inference latency.\"]\n",
        "* **Iterative Process:** üîÑ Machine learning projects are often iterative. Be prepared to revisit earlier phases as new insights emerge.\n",
        "\n",
        "---\n",
        "\n",
        "# üìö **Project Title:**\n",
        "**[Your Project Title Here, e.g., Customer Churn Prediction for Telecom X]**\n",
        "\n",
        "# üéØ **Project Goal:**\n",
        "**[Clearly state the overarching goal of this specific project, e.g., To reduce customer churn by 15% within the next 6 months by identifying at-risk customers early.]**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f735e41a",
      "metadata": {
        "id": "f735e41a"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 1: Problem Definition and Understanding*** üí°\n",
        "\n",
        "> _This initial phase is arguably the most critical. A clear understanding of the problem ensures your efforts are well-directed and aligned with business goals._\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uwvbLqkX4jfI",
      "metadata": {
        "id": "uwvbLqkX4jfI"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üí°**1.1 Define the Problem Clearly**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ML4udmgp5F8Q",
      "metadata": {
        "id": "ML4udmgp5F8Q"
      },
      "source": [
        "* **What exactly are you trying to solve?**\n",
        "  * e.g., Predicting which customers are most likely to cancel their subscription within the next month.\n",
        "* **Is it a classification, regression, clustering, or a different type of problem?**\n",
        "  * e.g., Binary Classification (Churn/No Churn).\n",
        "* **What are the inputs and desired outputs?**\n",
        "  * ***Inputs***: Customer demographic data, usage patterns, service history.\n",
        "  * ***Outputs***: Probability of churn for each customer.\n",
        "\n",
        "#### **Your Project Details**:\n",
        "\n",
        "* **Problem Statement:** [Add your specific problem statement here]\n",
        "* **Problem Type:** [e.g., Classification, Regression, etc.]\n",
        "* **Inputs & Outputs**:\n",
        "  * ***Inputs:*** [Describe your inputs]\n",
        "  * ***Outputs:*** [Describe your desired outputs]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xWfAETGl4FGh",
      "metadata": {
        "id": "xWfAETGl4FGh"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üí°**1.2 Establish Project Goals and Success Metrics**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D5LJDxw15bZn",
      "metadata": {
        "id": "D5LJDxw15bZn"
      },
      "source": [
        "* **What does \"success\" look like for this project?**\n",
        "  * *e.g., Achieving 90% precision in identifying churners, or reducing monthly churn rate by 5% over 3 months.*\n",
        "* **How will you measure performance?**\n",
        "  * *e.g., For classification: Accuracy, Precision, Recall, F1-score, AUC-ROC. For regression: RMSE, MAE, R-squared.*\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Quantifiable Success Metrics:**\n",
        "  * [Metric 1: e.g., Achieve X% Accuracy]\n",
        "  * [Metric 2: e.g., Reduce Y business metric by Z%]\n",
        "* **Business KPIs Impacted:** [List relevant business key performance indicators]\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EPErFxcQ4tOg",
      "metadata": {
        "id": "EPErFxcQ4tOg"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üí°**1.3 Identify Data Requirements and Sources**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amA-87w25uKG",
      "metadata": {
        "id": "amA-87w25uKG"
      },
      "source": [
        "* **What data do you need?**\n",
        "  * *e.g., Customer demographics, billing info, call logs, website activity.*\n",
        "* **Where will it come from?**\n",
        "  * *e.g., Internal CRM database, S3 bucket, third-party API.*\n",
        "* **Are there any privacy or regulatory constraints (e.g., GDPR, HIPAA)?**\n",
        "  * *e.g., Need to anonymize customer IDs.*\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Required Data & Sources:**\n",
        "    * [Data Source 1: Description, e.g., `customers_db.sql`]\n",
        "    * [Data Source 2: Description, e.g., `web_logs.csv`]\n",
        "* **Privacy/Regulatory Considerations:** [List any relevant concerns]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2sQIWm4D4x3Z",
      "metadata": {
        "id": "2sQIWm4D4x3Z"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üí°**1.4 Assess Feasibility and Resources**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CF-z6S8P5-Y6",
      "metadata": {
        "id": "CF-z6S8P5-Y6"
      },
      "source": [
        "* **Do you have the necessary data, computational resources (GPUs, cloud platforms), and expertise within the team?**\n",
        "  * *e.g., Yes, access to GCP compute engine, team has Python/Scikit-learn experience.*\n",
        "* **What are the timelines and budget constraints?**\n",
        "  * *e.g., 3-month timeline, limited GPU budget.*\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Available Resources:** [e.g., Team skills, compute resources, software licenses]\n",
        "* **Timelines & Budget:** [Specify project timeline and budget constraints]\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a891c464",
      "metadata": {
        "id": "a891c464"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 2: Data Collection and Preparation*** üìä\n",
        "\n",
        "> _This phase focuses on acquiring, cleaning, and transforming the data into a usable format for machine learning models. It often consumes a significant portion of project time._\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u1qbDzyC8nV1",
      "metadata": {
        "id": "u1qbDzyC8nV1"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üìä **2.1 Data Collection**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fZjSZF289AMk",
      "metadata": {
        "id": "fZjSZF289AMk"
      },
      "source": [
        "* **Gather data from identified sources.**\n",
        "* **Ensure data quantity and quality are sufficient.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6WnjjCWb9rb9",
      "metadata": {
        "id": "6WnjjCWb9rb9"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data collection (e.g., pandas, sqlalchemy)\n",
        "import pandas as pd\n",
        "# import sqlalchemy\n",
        "\n",
        "# Example: Load data from a CSV file\n",
        "try:\n",
        "    df_raw = pd.read_csv('path/to/your/raw_data.csv')\n",
        "    print(\"Raw data loaded successfully. Shape:\", df_raw.shape)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Raw data file not found. Please check the path.\")\n",
        "\n",
        "# Example: Connect to a database and fetch data\n",
        "# db_connection_str = 'mysql+mysqlconnector://user:password@host/db_name'\n",
        "# db_connection = sqlalchemy.create_engine(db_connection_str)\n",
        "# df_raw = pd.read_sql(\"SELECT * FROM your_table\", db_connection)\n",
        "# print(\"Data loaded from database. Shape:\", df_raw.shape)\n",
        "\n",
        "# Display a sample of the raw data\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_ByRTJg4FHj_",
      "metadata": {
        "id": "_ByRTJg4FHj_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OvxaCvfk-ZMj",
      "metadata": {
        "id": "OvxaCvfk-ZMj"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üìä **2.2 Data Cleaning**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1l9uAQns-kWb",
      "metadata": {
        "id": "1l9uAQns-kWb"
      },
      "source": [
        "* **Handle Missing Values:** Impute, drop rows/columns.\n",
        "* **Remove Duplicates:** Identify and eliminate redundant entries.\n",
        "* **Correct Inconsistent Data:** Standardize formats, fix typos.\n",
        "* **Deal with Outliers:** Identify and decide how to handle.\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Qk93Fo5-7H6",
      "metadata": {
        "id": "7Qk93Fo5-7H6"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values before cleaning:\")\n",
        "print(df_raw.isnull().sum()[df_raw.isnull().sum() > 0])\n",
        "\n",
        "# Example: Impute missing numerical values with the mean\n",
        "# df_raw['numerical_col'].fillna(df_raw['numerical_col'].mean(), inplace=True)\n",
        "\n",
        "# Example: Drop rows with missing values in critical columns\n",
        "# df_clean = df_raw.dropna(subset=['critical_col1', 'critical_col2'])\n",
        "\n",
        "# Check for duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows before cleaning: {df_raw.duplicated().sum()}\")\n",
        "# Example: Remove duplicate rows\n",
        "# df_clean = df_raw.drop_duplicates()\n",
        "\n",
        "# Example: Standardize a categorical column\n",
        "# df_clean['category_col'] = df_clean['category_col'].str.lower().replace({'us': 'usa', 'united states': 'usa'})\n",
        "\n",
        "# Outlier handling (e.g., using IQR or Z-score for numerical features)\n",
        "# Q1 = df_clean['numerical_col'].quantile(0.25)\n",
        "# Q3 = df_clean['numerical_col'].quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# lower_bound = Q1 - 1.5 * IQR\n",
        "# upper_bound = Q3 + 1.5 * IQR\n",
        "# df_clean = df_clean[(df_clean['numerical_col'] >= lower_bound) & (df_clean['numerical_col'] <= upper_bound)]\n",
        "\n",
        "# After cleaning, update the DataFrame used for further steps (e.g., df_clean = df_raw.copy() at the start of cleaning)\n",
        "df_clean = df_raw.copy() # Placeholder: ensure df_clean is defined for subsequent steps\n",
        "\n",
        "print(\"\\nData cleaning steps completed.\")\n",
        "print(f\"Shape after cleaning: {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPwCdgp__LzC",
      "metadata": {
        "id": "ZPwCdgp__LzC"
      },
      "source": [
        "* **Summary of Cleaning Actions:** [Describe what you did to clean the data]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z86aJagl_q1R",
      "metadata": {
        "id": "z86aJagl_q1R"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üìä **2.3 Data Integration (if necessary)**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0BiNqcNB_yuZ",
      "metadata": {
        "id": "0BiNqcNB_yuZ"
      },
      "source": [
        "* **Combine data from multiple sources.**\n",
        "* **Ensure consistent keys and formats.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uJOvhgFEAANB",
      "metadata": {
        "id": "uJOvhgFEAANB"
      },
      "outputs": [],
      "source": [
        "# Example: Merge multiple dataframes if applicable\n",
        "# df_integrated = pd.merge(df_clean, df_additional_data, on='customer_id', how='left')\n",
        "# print(f\"Shape after integration: {df_integrated.shape}\")\n",
        "\n",
        "df_integrated = df_clean.copy() # Placeholder if no integration is needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D1MAzATRAJQY",
      "metadata": {
        "id": "D1MAzATRAJQY"
      },
      "source": [
        "* **Integration Strategy:** [Explain how you integrated data, e.g., \"Merged customer demographics with transaction logs on customer_id.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AWkNH9n9AhBB",
      "metadata": {
        "id": "AWkNH9n9AhBB"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üìä **2.4 Data Transformation**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PRxbvv3OApzo",
      "metadata": {
        "id": "PRxbvv3OApzo"
      },
      "source": [
        "* **Feature Scaling:** Normalize (Min-Max) or standardize (Z-score) numerical features.\n",
        "* **Encoding Categorical Variables:** One-hot encoding, label encoding, target encoding.\n",
        "* **Feature Engineering:** Create new features from existing ones.\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4b3GLjA4ea",
      "metadata": {
        "id": "3c4b3GLjA4ea"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numerical_features = df_integrated.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = df_integrated.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessor for numerical and categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features), # or MinMaxScaler()\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Example: Apply transformations\n",
        "# This typically happens within a pipeline during model training,\n",
        "# but for standalone transformation for EDA or direct use:\n",
        "# X_transformed = preprocessor.fit_transform(df_integrated)\n",
        "\n",
        "# Example of manual feature engineering\n",
        "# df_transformed = df_integrated.copy()\n",
        "# df_transformed['age_at_signup'] = (pd.to_datetime('today') - pd.to_datetime(df_transformed['signup_date'])).dt.days / 365.25\n",
        "# df_transformed['monthly_avg_spend'] = df_transformed['total_spend'] / df_transformed['months_as_customer']\n",
        "\n",
        "df_transformed = df_integrated.copy() # Ensure df_transformed is defined\n",
        "print(\"\\nData transformation steps completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3hUpxhXA67P",
      "metadata": {
        "id": "Z3hUpxhXA67P"
      },
      "source": [
        "* **Transformation Summary:** [Describe your scaling, encoding, and engineered features.]\n",
        "  * e.g., \"Applied StandardScaler to numerical features; OneHotEncoded 'gender' and 'contract_type'; Created 'tenure_months' feature.\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vUZWqzkBC-ih",
      "metadata": {
        "id": "vUZWqzkBC-ih"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üìä **2.5 Data Splitting**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65iDxQE5DIAe",
      "metadata": {
        "id": "65iDxQE5DIAe"
      },
      "source": [
        "* **Divide the dataset into training, validation (optional but recommended), and test sets.**\n",
        "* **Maintain class distribution using stratification if dealing with imbalanced datasets.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pyXR6NdCDaeB",
      "metadata": {
        "id": "pyXR6NdCDaeB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your target variable (y) and features (X)\n",
        "TARGET_COLUMN = 'churn' # Replace with your actual target column name\n",
        "X = df_transformed.drop(columns=[TARGET_COLUMN])\n",
        "y = df_transformed[TARGET_COLUMN]\n",
        "\n",
        "# Split data into training and temporary (validation + test) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Split temporary data into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "# Verify stratification (especially important for imbalanced datasets)\n",
        "print(\"\\nTarget distribution in splits:\")\n",
        "print(f\"Train: {y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Validation: {y_val.value_counts(normalize=True)}\")\n",
        "print(f\"Test: {y_test.value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PaZPzOriDc0Q",
      "metadata": {
        "id": "PaZPzOriDc0Q"
      },
      "source": [
        "* **Splitting Strategy:** [e.g., \"70/15/15 train/validation/test split, stratified by churn status.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4eb4af",
      "metadata": {
        "id": "2a4eb4af"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 3: Exploratory Data Analysis (EDA)*** üîé\n",
        "\n",
        "> _EDA is crucial for understanding the data's characteristics, identifying patterns, and gaining insights that inform model selection and feature engineering._\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lClqgcmPGe9l",
      "metadata": {
        "id": "lClqgcmPGe9l"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üîé **3.1 Understand Data Distribution**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlKp9_fOHMi9",
      "metadata": {
        "id": "VlKp9_fOHMi9"
      },
      "source": [
        "* **Summary statistics, histograms, box plots for numerical features.**\n",
        "* **Bar charts for categorical features.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R0nZPnJVHbj-",
      "metadata": {
        "id": "R0nZPnJVHbj-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display descriptive statistics for numerical features\n",
        "print(\"Descriptive Statistics for Numerical Features:\")\n",
        "print(X_train[numerical_features].describe())\n",
        "\n",
        "# Plot histograms for numerical features\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_features):\n",
        "    plt.subplot(len(numerical_features)//3 + 1, 3, i + 1)\n",
        "    sns.histplot(X_train[col], kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot bar charts for categorical features (top N categories)\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(categorical_features):\n",
        "    if i >= 6: break # Limit for display purposes\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    sns.countplot(y=X_train[col], order=X_train[col].value_counts().index)\n",
        "    plt.title(f'Count of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cBXrNMIUHd1W",
      "metadata": {
        "id": "cBXrNMIUHd1W"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JnWthA9tHmCb",
      "metadata": {
        "id": "JnWthA9tHmCb"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üîé **3.2 Identify Relationships Between Variables**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ti56WvLdH4Kf",
      "metadata": {
        "id": "ti56WvLdH4Kf"
      },
      "source": [
        "* **Correlation matrices for numerical features.**\n",
        "* **Scatter plots to visualize relationships.**\n",
        "* **Crosstabs for categorical variables.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PGNRAcqiIE-L",
      "metadata": {
        "id": "PGNRAcqiIE-L"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix for numerical features\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(X_train[numerical_features].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "# Example: Scatter plot of two numerical features vs. target\n",
        "# sns.scatterplot(data=X_train, x='feature_A', y='feature_B', hue=y_train)\n",
        "# plt.title('Feature A vs. Feature B colored by Target')\n",
        "# plt.show()\n",
        "\n",
        "# Example: Box plot of a numerical feature vs. categorical target\n",
        "# sns.boxplot(x=y_train, y=X_train['numerical_feature_X'])\n",
        "# plt.title('Numerical Feature X distribution across Target Classes')\n",
        "# plt.show()\n",
        "\n",
        "# Crosstab for categorical features vs. target\n",
        "# for cat_col in categorical_features:\n",
        "#     print(f\"\\nCrosstab for {cat_col} vs. {TARGET_COLUMN}:\")\n",
        "#     print(pd.crosstab(X_train[cat_col], y_train, normalize='index'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1jBNet8eIHIU",
      "metadata": {
        "id": "1jBNet8eIHIU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brgzFzjrINyl",
      "metadata": {
        "id": "brgzFzjrINyl"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üîé **3.3 Visualize Data**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cXbmfV2ISFD",
      "metadata": {
        "id": "3cXbmfV2ISFD"
      },
      "source": [
        "* **Use various plots to uncover patterns, trends, and anomalies.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xEExqpfRIYqj",
      "metadata": {
        "id": "xEExqpfRIYqj"
      },
      "outputs": [],
      "source": [
        "# Example: Pairplot for a subset of features (can be very slow for many features)\n",
        "# sns.pairplot(pd.concat([X_train[numerical_features[:3]], y_train], axis=1), hue=TARGET_COLUMN)\n",
        "# plt.show()\n",
        "\n",
        "# Example: Custom visualizations based on insights\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.violinplot(x=y_train, y=X_train['feature_Y'])\n",
        "# plt.title('Violin Plot of Feature Y by Target')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccsAKtGPIZ4Y",
      "metadata": {
        "id": "ccsAKtGPIZ4Y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YA0LD8vEIkHO",
      "metadata": {
        "id": "YA0LD8vEIkHO"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üîé **3.4 Detect and Handle Anomalies/Outliers (revisit if necessary)**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C63Uv_pRIqrN",
      "metadata": {
        "id": "C63Uv_pRIqrN"
      },
      "source": [
        "* **Further investigation of unusual data points.**\n",
        "\n",
        "#### **Your Project Notes:**\n",
        "\n",
        "* **Anomalies Found:** [Describe any significant anomalies/outliers observed]\n",
        "* **Handling Strategy:** [How did you decide to handle them? e.g., \"Decided to keep outliers as they represent genuine edge cases.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6747c19d",
      "metadata": {
        "id": "6747c19d"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 4: Model Selection and Training*** üß†\n",
        "\n",
        "> _This phase involves choosing appropriate algorithms, training them on the prepared data, and tuning their parameters._\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mtytVHIfLsUS",
      "metadata": {
        "id": "mtytVHIfLsUS"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üß† **4.1 Choose Appropriate Algorithms**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ZwPxvoQtvE",
      "metadata": {
        "id": "38ZwPxvoQtvE"
      },
      "source": [
        "* **Based on the problem type and data characteristics.**\n",
        "* **Consider simplicity first before moving to more complex models.**\n",
        "\n",
        "#### **Your Project Notes:**\n",
        "\n",
        "* **[Model 1: e.g., Logistic Regression (Baseline)]**\n",
        "* **[Model 2: e.g., RandomForestClassifier]**\n",
        "* **[Model 3: e.g., GradientBoostingClassifier (XGBoost/LightGBM)]**\n",
        "* **[Model 4: e.g., SVM, Neural Network (if justified)]**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XskVMOVqL3Fz",
      "metadata": {
        "id": "XskVMOVqL3Fz"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üß† **4.2 Train Models**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wfz-zVBgRatv",
      "metadata": {
        "id": "wfz-zVBgRatv"
      },
      "source": [
        "* **Fit chosen algorithms to the training data.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AgXXSndnRo0K",
      "metadata": {
        "id": "AgXXSndnRo0K"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Re-define preprocessor (if not already done for a consistent pipeline)\n",
        "# Ensure numerical_features and categorical_features are correctly identified from X_train\n",
        "numerical_features_final = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features_final = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "preprocessor_final = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features_final),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_final)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns if any\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': Pipeline(steps=[('preprocessor', preprocessor_final),\n",
        "                                            ('classifier', LogisticRegression(random_state=42, solver='liblinear'))]),\n",
        "    'Random Forest': Pipeline(steps=[('preprocessor', preprocessor_final),\n",
        "                                      ('classifier', RandomForestClassifier(random_state=42))]),\n",
        "    'Gradient Boosting': Pipeline(steps=[('preprocessor', preprocessor_final),\n",
        "                                        ('classifier', GradientBoostingClassifier(random_state=42))])\n",
        "}\n",
        "\n",
        "# Train models\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    trained_models[name] = model\n",
        "    print(f\"{name} trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wBNX2FFORrDJ",
      "metadata": {
        "id": "wBNX2FFORrDJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yg6edh0cL9nh",
      "metadata": {
        "id": "yg6edh0cL9nh"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üß† **4.3 Hyperparameter Tuning**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jhS1RkiOR5-7",
      "metadata": {
        "id": "jhS1RkiOR5-7"
      },
      "source": [
        "* **Optimize model performance by adjusting hyperparameters.**\n",
        "* **Use techniques like Grid Search, Random Search, or Bayesian Optimization.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L6vMJ5rGSGQ8",
      "metadata": {
        "id": "L6vMJ5rGSGQ8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Example: Hyperparameter tuning for Random Forest\n",
        "print(\"\\nPerforming Hyperparameter Tuning for Random Forest...\")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV (can be replaced with RandomizedSearchCV for larger search spaces)\n",
        "grid_search_rf = GridSearchCV(trained_models['Random Forest'], param_grid_rf, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
        "print(f\"Best F1-score on training set for Random Forest: {grid_search_rf.best_score_:.4f}\")\n",
        "\n",
        "# Update the trained model with the best estimator\n",
        "trained_models['Random Forest (Tuned)'] = grid_search_rf.best_estimator_\n",
        "\n",
        "# Perform tuning for other models as well\n",
        "# param_grid_lr = {\n",
        "#     'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
        "# }\n",
        "# grid_search_lr = GridSearchCV(trained_models['Logistic Regression'], param_grid_lr, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
        "# grid_search_lr.fit(X_train, y_train)\n",
        "# print(f\"Best parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
        "# trained_models['Logistic Regression (Tuned)'] = grid_search_lr.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "srF0zY1YSJxT",
      "metadata": {
        "id": "srF0zY1YSJxT"
      },
      "source": [
        "* **Tuning Strategy:** [e.g., \"Used GridSearchCV with 3-fold cross-validation, optimizing for F1-score.\"]\n",
        "* **Best Hyperparameters Found:** [List the best parameters for your chosen models]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z3MDv6KvMCSr",
      "metadata": {
        "id": "z3MDv6KvMCSr"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üß† **4.4 Cross-Validation**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wqjmAcUBSapy",
      "metadata": {
        "id": "wqjmAcUBSapy"
      },
      "source": [
        "* **Use techniques like k-fold cross-validation to get a more robust estimate of model performance.**\n",
        "\n",
        "#### **Your Project Notes:**\n",
        "\n",
        "* **Cross-Validation Details:** [e.g., \"Performed 5-fold stratified cross-validation on the training set during hyperparameter tuning.\"]\n",
        "* **Observed Stability:** [Comment on the stability of scores across folds.]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2db48e",
      "metadata": {
        "id": "bf2db48e"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 5: Model Evaluation*** ‚≠ê\n",
        "\n",
        "> _Evaluate the performance of the trained models using the defined success metrics on unseen data._\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65i64UHrNCr4",
      "metadata": {
        "id": "65i64UHrNCr4"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### ‚≠ê **5.1 Evaluate on Validation Set (if applicable)**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_dKNwBE6S596",
      "metadata": {
        "id": "_dKNwBE6S596"
      },
      "source": [
        "* **Use the validation set to compare different models and fine-tune hyperparameters.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amld_g6mTBEh",
      "metadata": {
        "id": "amld_g6mTBEh"
      },
      "outputs": [],
      "source": [
        "# Evaluate all trained models on the validation set\n",
        "results = {}\n",
        "for name, model in trained_models.items():\n",
        "    y_pred_val = model.predict(X_val)\n",
        "    y_prob_val = model.predict_proba(X_val)[:, 1] # Probability for the positive class\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred_val)\n",
        "    precision = precision_score(y_val, y_pred_val)\n",
        "    recall = recall_score(y_val, y_pred_val)\n",
        "    f1 = f1_score(y_val, y_pred_val)\n",
        "    roc_auc = roc_auc_score(y_val, y_prob_val)\n",
        "\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    }\n",
        "    print(f\"\\n--- {name} Validation Metrics ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Convert results to a DataFrame for easy comparison\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Performance on Validation Set:\")\n",
        "print(results_df)\n",
        "\n",
        "# Based on validation set, choose the best model for final evaluation\n",
        "best_model_name_val = results_df['ROC AUC'].idxmax() # Or another metric based on your goal\n",
        "final_model = trained_models[best_model_name_val]\n",
        "print(f\"\\nBest model based on validation ROC AUC: {best_model_name_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QV0WA6qzTD6y",
      "metadata": {
        "id": "QV0WA6qzTD6y"
      },
      "source": [
        "* **Validation Set Performance Summary:** [Summarize key metrics and insights.]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TiY6iTXMNN5n",
      "metadata": {
        "id": "TiY6iTXMNN5n"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### ‚≠ê **5.2 Evaluate on Test Set**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0EWlwDGTMdL",
      "metadata": {
        "id": "b0EWlwDGTMdL"
      },
      "source": [
        "* **Use the completely unseen test set to get an unbiased estimate of the model's generalization performance.**\n",
        "* **Calculate chosen metrics, generate confusion matrices.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2NHAKKKaTTWC",
      "metadata": {
        "id": "2NHAKKKaTTWC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "print(f\"\\n--- Evaluating Final Model ({best_model_name_val}) on Test Set ---\")\n",
        "\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "y_prob_test = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "test_precision = precision_score(y_test, y_pred_test)\n",
        "test_recall = recall_score(y_test, y_pred_test)\n",
        "test_f1 = f1_score(y_test, y_pred_test)\n",
        "test_roc_auc = roc_auc_score(y_test, y_prob_test)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(f'Confusion Matrix for {best_model_name_val} on Test Set')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {test_roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(f'Receiver Operating Characteristic for {best_model_name_val}')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seB3kQw5TUn5",
      "metadata": {
        "id": "seB3kQw5TUn5"
      },
      "source": [
        "* **Test Set Performance Summary:** [Final reported metrics and interpretation.]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4gGT9f93NSRY",
      "metadata": {
        "id": "4gGT9f93NSRY"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### ‚≠ê **5.3 Interpret Model Results**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gAc0KLbeTafC",
      "metadata": {
        "id": "gAc0KLbeTafC"
      },
      "source": [
        "* **Understand why the model is making certain predictions (e.g., feature importance, SHAP values, LIME).**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FcMFms3aTgWS",
      "metadata": {
        "id": "FcMFms3aTgWS"
      },
      "outputs": [],
      "source": [
        "# Example: Feature Importance for tree-based models\n",
        "if hasattr(final_model.named_steps['classifier'], 'feature_importances_'):\n",
        "    feature_importances = final_model.named_steps['classifier'].feature_importances_\n",
        "    # Get feature names after one-hot encoding if applicable\n",
        "    ohe_feature_names = final_model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features_final)\n",
        "    all_feature_names = numerical_features_final + list(ohe_feature_names) # Combine feature names\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': all_feature_names,\n",
        "        'Importance': feature_importances\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 Feature Importances:\")\n",
        "    print(importance_df.head(10))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
        "    plt.title('Top 10 Feature Importances')\n",
        "    plt.show()\n",
        "\n",
        "# If using more complex models, consider libraries like SHAP or LIME\n",
        "# import shap\n",
        "# explainer = shap.TreeExplainer(final_model.named_steps['classifier'])\n",
        "# shap_values = explainer.shap_values(preprocessor_final.transform(X_test))\n",
        "# shap.summary_plot(shap_values, preprocessor_final.transform(X_test), feature_names=all_feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uhDqY1OlTjWU",
      "metadata": {
        "id": "uhDqY1OlTjWU"
      },
      "source": [
        "* **Key Findings from Interpretation:** [e.g., \"Customer tenure and monthly charges are the most influential features for churn prediction.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gz7-G7CONXs_",
      "metadata": {
        "id": "Gz7-G7CONXs_"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### ‚≠ê **5.4 Compare Models**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GHW1uMrXTpQQ",
      "metadata": {
        "id": "GHW1uMrXTpQQ"
      },
      "source": [
        "* **If multiple models were trained, compare their performance and choose the best one.**\n",
        "\n",
        "#### **Your Project Notes:**\n",
        "\n",
        "* **Final Model Choice:** [State which model was chosen and why (e.g., \"Gradient Boosting Classifier chosen due to superior F1-score and AUC on the test set, balancing precision and recall.\")]\n",
        "* **Trade-offs:** [Discuss any trade-offs considered, e.g., \"Logistic Regression was faster but less accurate, while Gradient Boosting offered better performance at the cost of interpretability.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d258c188",
      "metadata": {
        "id": "d258c188"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 6: Deployment*** üöÄ\n",
        "\n",
        "> _Making the model available for practical use, often in a production environment._\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y93bNlYkND0Q",
      "metadata": {
        "id": "y93bNlYkND0Q"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üöÄ **6.1 Model Serialization**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LVQlSFh8T_Wx",
      "metadata": {
        "id": "LVQlSFh8T_Wx"
      },
      "source": [
        "* **Save the trained model in a deployable format.**\n",
        "\n",
        "#### **Your Project Code & Details:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A0YDi9qJUEAI",
      "metadata": {
        "id": "A0YDi9qJUEAI"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create a directory for saving the model\n",
        "MODEL_DIR = 'model_artifacts'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Save the final model (the entire pipeline if using one)\n",
        "model_path = os.path.join(MODEL_DIR, 'final_ml_model.joblib')\n",
        "joblib.dump(final_model, model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# To load the model later:\n",
        "# loaded_model = joblib.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wm8bFYHRUFxx",
      "metadata": {
        "id": "Wm8bFYHRUFxx"
      },
      "source": [
        "* **Serialization Format:** [e.g., \"joblib\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fMO5h_glN1SH",
      "metadata": {
        "id": "fMO5h_glN1SH"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üöÄ **6.2 API Development (if applicable)**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KEzJJd6uULwv",
      "metadata": {
        "id": "KEzJJd6uULwv"
      },
      "source": [
        "* **Wrap the model in a RESTful API for easy integration.**\n",
        "\n",
        "#### **Your Project Notes & Pseudocode:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cQtS3j30URwX",
      "metadata": {
        "id": "cQtS3j30URwX"
      },
      "outputs": [],
      "source": [
        "# Example of a simple Flask API structure (pseudocode)\n",
        "# from flask import Flask, request, jsonify\n",
        "# import joblib\n",
        "#\n",
        "# app = Flask(__name__)\n",
        "#\n",
        "# # Load the model when the app starts\n",
        "# model = joblib.load('model_artifacts/final_ml_model.joblib')\n",
        "#\n",
        "# @app.route('/predict', methods=['POST'])\n",
        "# def predict():\n",
        "#     data = request.get_json(force=True)\n",
        "#     # Assuming data is a dictionary matching your feature schema\n",
        "#     input_df = pd.DataFrame([data])\n",
        "#     prediction = model.predict(input_df)[0]\n",
        "#     probability = model.predict_proba(input_df)[0].tolist()\n",
        "#     return jsonify({'prediction': int(prediction), 'probability': probability})\n",
        "#\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(debug=True, host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KBz85uemUU5Q",
      "metadata": {
        "id": "KBz85uemUU5Q"
      },
      "source": [
        "* **API Framework:** [e.g., Flask, FastAPI]\n",
        "* **Endpoint Details:** [e.g., /predict for POST requests, expected JSON payload.]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6yZjUGEtN5MO",
      "metadata": {
        "id": "6yZjUGEtN5MO"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üöÄ **6.3 Deployment Environment**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7g6MT974UcqC",
      "metadata": {
        "id": "7g6MT974UcqC"
      },
      "source": [
        "* **Deploy the model to a production environment.**\n",
        "* **Consider containerization (Docker, Kubernetes).**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Deployment Platform:** [e.g., AWS Sagemaker, Google Cloud AI Platform, Azure ML, Kubernetes cluster, local server]\n",
        "* **Containerization Strategy:** [e.g., \"Docker image created for the Flask API.\"]\n",
        "* **Key Environment Variables/Configurations:** [List crucial environment variables]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vtc40SLNN8dW",
      "metadata": {
        "id": "Vtc40SLNN8dW"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üöÄ **6.4 Infrastructure Setup**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "juzH2PgwUrCB",
      "metadata": {
        "id": "juzH2PgwUrCB"
      },
      "source": [
        "* **Set up necessary infrastructure for model serving, scalability, and monitoring.**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Infrastructure Components:** [e.g., Load balancer, auto-scaling groups, database connections for real-time data.]\n",
        "* **Scaling Strategy:** [e.g., \"Auto-scaling based on CPU utilization.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EmYBPPr1BR54",
      "metadata": {
        "id": "EmYBPPr1BR54"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## ***Phase 7: Monitoring and Maintenance*** üõ†Ô∏è\n",
        "\n",
        "> _Machine learning models are not \"set and forget.\" They require continuous monitoring and periodic updates._\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyR0xjIzNE6H",
      "metadata": {
        "id": "tyR0xjIzNE6H"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üõ†Ô∏è **7.1 Monitor Model Performance**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DbmGgKWpVP24",
      "metadata": {
        "id": "DbmGgKWpVP24"
      },
      "source": [
        "* **Track key performance metrics in real-time.**\n",
        "* **Set up alerts for performance degradation.**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Monitoring Tools:** [e.g., Prometheus, Grafana, AWS CloudWatch, custom dashboards.]\n",
        "* **Metrics Tracked:** [e.g., Accuracy, F1-score (re-calculated on live data), latency, request throughput.]\n",
        "* **Alerting Mechanism:** [e.g., Slack notifications if F1-score drops below X for Y hours.]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BdZ4TqCIOIvW",
      "metadata": {
        "id": "BdZ4TqCIOIvW"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üõ†Ô∏è **7.2 Monitor Data Drift**\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tavmWmzvXCTu",
      "metadata": {
        "id": "tavmWmzvXCTu"
      },
      "source": [
        "* **Detect changes in the distribution of input data over time.**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Drift Detection Method:** [e.g., Kolmogorov-Smirnov test, Population Stability Index (PSI), adversarial validation.]\n",
        "* **Drift Triggers:** [e.g., \"If distribution of 'customer_age' changes significantly (e.g., PSI > 0.1) trigger an alert.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bq9OIWg1OMZJ",
      "metadata": {
        "id": "bq9OIWg1OMZJ"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üõ†Ô∏è **7.3 Retraining Strategy**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G8s9NvPLVoZf",
      "metadata": {
        "id": "G8s9NvPLVoZf"
      },
      "source": [
        "* **Establish a schedule or trigger for retraining the model.**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Retraining Frequency:** [e.g., \"Monthly retrain with the latest 6 months of data.\"]\n",
        "* **Retraining Triggers:** [e.g., \"Retrain if model performance drops by 5% on live data, or if significant data drift is detected.\"]\n",
        "* **Automated Pipeline:** [e.g., \"CI/CD pipeline for automated retraining and deployment.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1xgeMyNJOPX-",
      "metadata": {
        "id": "1xgeMyNJOPX-"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üõ†Ô∏è **7.4 Model Versioning**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fg3M9x5UVyfK",
      "metadata": {
        "id": "Fg3M9x5UVyfK"
      },
      "source": [
        "* **Maintain different versions of models and track their performance.**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Versioning System:** [e.g., MLflow, DVC (Data Version Control), custom naming conventions.]\n",
        "* **Rollback Plan:** [e.g., \"Ability to roll back to previous model version if new deployment fails or underperforms.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cBLIMaQmOUmW",
      "metadata": {
        "id": "cBLIMaQmOUmW"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "### üõ†Ô∏è **7.5 Logging and Auditing**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_gCFdrzLWGOv",
      "metadata": {
        "id": "_gCFdrzLWGOv"
      },
      "source": [
        "* **Log model predictions, inputs, and outputs for debugging and auditing.**\n",
        "\n",
        "#### **Your Project Details:**\n",
        "\n",
        "* **Logging Strategy:** [e.g., \"Log all prediction requests and responses to a dedicated log sink (e.g., Stackdriver, Splunk).\"]\n",
        "* **Audit Trail:** [e.g., \"Ensure PII is masked in logs; maintain an audit trail of model updates and deployments.\"]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "TecP0DwmxK3u"
      },
      "id": "TecP0DwmxK3u"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}